{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd3a38e7-bcee-4e8c-b4e8-64bd876f055e",
   "metadata": {},
   "source": [
    "**Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.**\n",
    "\n",
    "Simple linear regression is a statistical method that helps to establish a relationship between two continuous variables, where one variable is considered as the dependent variable, and the other variable is considered as the independent variable. The objective of the model is to estimate the value of the dependent variable based on the values of the independent variable. In contrast, multiple linear regression is a statistical technique used to establish the relationship between a dependent variable and two or more independent variables.\n",
    "\n",
    "Example of Simple Linear Regression: Consider a scenario where we want to predict the weight of a person based on their height. Here, height is considered as the independent variable, and weight is considered as the dependent variable.\n",
    "\n",
    "Example of Multiple Linear Regression: Suppose we want to predict the salary of an employee based on their age, education, and years of experience. Here, age, education, and years of experience are considered as the independent variables, and salary is considered as the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fad6c0-362a-4369-b37a-72c89051fcd6",
   "metadata": {},
   "source": [
    "**Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?**\n",
    "\n",
    "Linear regression assumptions are:\n",
    "\n",
    "- Linearity: There should be a linear relationship between the dependent and independent variables. \n",
    "- Independence: The observations should be independent of each other. \n",
    "- Homoscedasticity: The variance of the residuals should be constant across all levels of the independent variable. Normality: The residuals should follow a normal distribution.\n",
    "- No Multicollinearity: There should be no perfect multicollinearity between the independent variables. To check whether these assumptions hold in a given dataset, we can use diagnostic plots like a scatter plot, residuals vs fitted values plot, normal probability plot, and leverage plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470572f9-f5b5-44a6-ac05-cb08177fc45c",
   "metadata": {},
   "source": [
    "**Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.**\n",
    "\n",
    "In a linear regression model, the slope represents the change in the dependent variable associated with a one-unit increase in the independent variable. The intercept represents the predicted value of the dependent variable when the independent variable is zero.\n",
    "\n",
    "Example: Suppose we want to predict the sales of a product based on the advertising budget. Here, the slope of the regression line represents the increase in sales associated with an increase in the advertising budget by one unit. The intercept represents the predicted sales when there is no advertising."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0ea762-9be2-4042-8457-60b055e0cfe8",
   "metadata": {},
   "source": [
    "**Q4.Explain the concept of gradient descent. How is it used in machine learning?**\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function in machine learning. The cost function measures the difference between the predicted values and the actual values. The objective of gradient descent is to adjust the model parameters iteratively to reach the global minimum of the cost function.\n",
    "\n",
    "In machine learning, gradient descent is used to train models like linear regression, logistic regression, and neural networks. The algorithm calculates the gradient of the cost function with respect to the model parameters and updates the parameters in the opposite direction of the gradient until it reaches the minimum of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae9f8a-ecad-43e1-9d08-39f5d98a1573",
   "metadata": {},
   "source": [
    "**Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?**\n",
    "\n",
    "Multiple linear regression is a statistical method used to establish a relationship between a dependent variable and two or more independent variables. The model assumes that there is a linear relationship between the dependent variable and the independent variables. The model equation for multiple linear regression is given by:\n",
    "\n",
    "y = β0 + β1x1 + β2x2 + … + βnxn + ε\n",
    "\n",
    "where,\n",
    "\n",
    "y is the dependent variable x1, x2, …, xn are the independent variables β0, β1, β2, …, βn are the regression coefficients ε is the error term. Multiple linear regression differs from simple linear regression in that it involves more than one independent variable, whereas simple linear regression involves only one independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb300865-2d31-496d-aacc-26a25524c9e6",
   "metadata": {},
   "source": [
    "**Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?**\n",
    "\n",
    "Multicollinearity is a phenomenon in multiple linear regression where two or more independent variables are highly correlated with each other. This can lead to unstable and unreliable estimates of the regression coefficients, making it difficult to interpret the model.\n",
    "\n",
    "To detect multicollinearity, we can calculate the correlation matrix between the independent variables and check for high correlations. We can also use variance inflation factor (VIF) to quantify the severity of multicollinearity. A VIF value greater than 5 indicates a high level of multicollinearity.\n",
    "\n",
    "To address multicollinearity, we can use techniques like principal component analysis (PCA) or ridge regression, which can reduce the number of variables in the model or adjust the regression coefficients to mitigate the effect of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf348a-4a48-4c57-bd93-342105565297",
   "metadata": {},
   "source": [
    "**Q7. Describe the polynomial regression model. How is it different from linear regression?**\n",
    "\n",
    "Polynomial regression is a type of regression analysis in which the relationship between the dependent variable and the independent variable(s) is modeled as an nth-degree polynomial function. Unlike linear regression, which assumes a linear relationship between the dependent variable and independent variable(s), polynomial regression can capture non-linear relationships between the variables.\n",
    "\n",
    "The model equation for polynomial regression is given by:\n",
    "\n",
    "y = β0 + β1x + β2x^2 + … + βnx^n + ε\n",
    "\n",
    "where,\n",
    "\n",
    "y is the dependent variable x is the independent variable β0, β1, β2, …, βn are the regression coefficients ε is the error term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dc0373-5dcd-4974-90f6-10b03a23a36c",
   "metadata": {},
   "source": [
    "**Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?**\n",
    "\n",
    "The advantage of polynomial regression is that it can capture non-linear relationships between the variables, whereas linear regression can only model linear relationships. Additionally, polynomial regression can fit complex data patterns better than linear regression.\n",
    "\n",
    "The disadvantage of polynomial regression is that it can easily overfit the data if the degree of the polynomial is too high, leading to poor generalization on new data. Polynomial regression can also be computationally expensive and difficult to interpret.\n",
    "\n",
    "Polynomial regression is useful when the relationship between the dependent variable and independent variable(s) is not linear and can be approximated by a polynomial function. It can also be used when there are interactions between the independent variables that cannot be captured by linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b52b73-a292-467a-8ef9-5a98fb0f355a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
