{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d796032-d6ad-4029-a432-fadd407540ae",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe708c61-d6c7-4a76-bc84-6946f0890858",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for both regression and classification tasks. It is an ensemble learning method that combines the predictions of multiple weak learners (typically decision trees) to create a strong predictive model. The term \"gradient boosting\" refers to the optimization algorithm used to minimize the errors of the weak learners.\n",
    "\n",
    "Here's a high-level overview of how Gradient Boosting Regression works:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - The algorithm starts with a simple model, often a single decision tree, which serves as the initial predictor.\n",
    "\n",
    "2. **Sequential Learning:**\n",
    "   - The algorithm adds weak learners sequentially, each one correcting the errors made by the existing ensemble.\n",
    "   - A weak learner is typically a shallow decision tree (a tree with few nodes and leaves).\n",
    "\n",
    "3. **Gradient Descent Optimization:**\n",
    "   - The key idea is to fit each weak learner to the residual errors (the differences between the actual and predicted values) of the current ensemble.\n",
    "   - The new weak learner is trained to predict the residuals, and its predictions are added to the ensemble.\n",
    "\n",
    "4. **Weighted Combination:**\n",
    "   - Each weak learner is assigned a weight based on its contribution to minimizing the overall error.\n",
    "   - The final prediction is a weighted sum of the predictions from all the weak learners.\n",
    "\n",
    "5. **Stopping Criteria:**\n",
    "   - The process continues until a predefined number of weak learners is reached or until a certain level of performance is achieved.\n",
    "\n",
    "Gradient Boosting Regression has several hyperparameters that can be tuned, such as the learning rate, tree depth, and the number of trees. Common implementations of Gradient Boosting include XGBoost, LightGBM, and scikit-learn's GradientBoostingRegressor.\n",
    "\n",
    "This technique often produces highly accurate models and is robust against overfitting. However, it may require careful hyperparameter tuning, and the training process can be computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4d92c8-c35a-4099-a815-fd21f3d10e77",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f729da79-0c89-4b65-ac84-767daf66154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    \n",
    "    def __init__(self, n_trees=100, max_depth=3, learning_rate=0.1):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trees = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.full(np.shape(y), np.mean(y))\n",
    "        for i in range(self.n_trees):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            residuals = y - y_pred\n",
    "            tree.fit(X, residuals)\n",
    "            update = tree.predict(X) * self.learning_rate\n",
    "            y_pred += update\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for tree in self.trees:\n",
    "            y_pred += tree.predict(X) * self.learning_rate\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39981a98-ce8d-4940-a782-949d1e3ebedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# generate sample data\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1)\n",
    "\n",
    "# split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# fit the gradient boosting model\n",
    "gbr = GradientBoostingRegressor(n_trees=100, max_depth=3, learning_rate=0.1)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = gbr.predict(X_test)\n",
    "\n",
    "# evaluate the performance of the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e6c9ac-cc5e-4c36-84c4-e3d16e52b1f1",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf25372-533c-40f1-a383-c80538237202",
   "metadata": {},
   "source": [
    "When optimizing the performance of a Gradient Boosting Regression model, hyperparameter tuning is crucial. Two common methods for hyperparameter tuning are grid search and random search. Here, I'll provide an example using scikit-learn's GradientBoostingRegressor with grid search. Please note that you can adapt the code for other libraries like XGBoost or LightGBM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20feae9f-7de4-4de4-a735-7d8289b2e2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "Mean Squared Error on Test Set: 1393.633744803083\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming you have your features (X) and target variable (y) ready\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create the GradientBoostingRegressor model\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on Test Set: {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2571a9a-dc21-41ab-b1c8-4f0be27f18c6",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "- `param_grid` defines the grid of hyperparameter values to search. You can adjust the ranges and add more hyperparameters based on your specific use case.\n",
    "- `GridSearchCV` performs a search over the specified parameter values using cross-validation to find the combination that gives the best performance.\n",
    "- The best hyperparameters and the corresponding model are printed, and the model is evaluated on the test set using mean squared error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ebc58-c4c5-471d-953b-c1a1041f29ec",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c789dc7a-2b33-4fc7-8dfc-0b4f6deeb77c",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a weak learner refers to a model that performs slightly better than random chance on a given task. Specifically, it is a model that has limited predictive power and is only slightly better than random guessing for the problem at hand.\n",
    "\n",
    "In the case of Gradient Boosting, decision trees are commonly used as weak learners. These are often shallow trees with a small number of nodes and leaves. Shallow trees are simpler and less expressive, which makes them weak learners. Each tree in the ensemble contributes a small amount to the overall predictive power of the model.\n",
    "\n",
    "The strength of Gradient Boosting comes from the combination of these weak learners. The algorithm sequentially adds weak learners to the ensemble, and each new learner is trained to correct the errors made by the existing ensemble. By focusing on the mistakes of the current model, Gradient Boosting builds a strong predictive model by iteratively improving upon the weaknesses of the previous models.\n",
    "\n",
    "The term \"weak learner\" is used in contrast to a \"strong learner,\" which is a model that performs well on its own without the need for ensembling. The idea behind using weak learners in ensemble methods like Gradient Boosting is that, even though individual weak learners may not be very accurate, their collective strength, when combined in an ensemble, can lead to a highly accurate and robust predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a57bf8-8ccd-4e37-ba0e-51c807f84603",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2e6c00-f3bd-43e9-a710-fae759792b68",
   "metadata": {},
   "source": [
    "The intuition behind Gradient Boosting is to iteratively fit a sequence of weak learners to the data, each one correcting the mistakes of the previous one. The idea is to build an ensemble of models that can capture complex patterns in the data by combining the individual strengths of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05da02a-5b1f-4e77-8cb5-a474945372ac",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0dcd3-56ca-4a84-b0a4-935b56c4a896",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners by sequentially fitting a new model to the residuals of the previous model. At each iteration, the algorithm calculates the difference between the true target values and the predicted values of the current model. This difference, known as the residual, becomes the new target for the next model. The new model is then trained on the residuals, and its predictions are added to the predictions of the previous models, weighted by a learning rate parameter that controls the contribution of each model. This process is repeated until the desired number of models is reached or until the residuals converge to a minimum value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e8271-1f81-48c0-a2a9-b2eecc07d76c",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb9806e-816b-4f5c-9d7d-bea09ac950ad",
   "metadata": {},
   "source": [
    "The steps involved in constructing the mathematical intuition of Gradient Boosting algorithm are:\n",
    "\n",
    "Initialize the model with a constant value, usually the mean of the target variable.\n",
    "\n",
    "For each iteration:\n",
    "\n",
    "a. Calculate the negative gradient of the loss function with respect to the current predictions.\n",
    "\n",
    "b. Fit a weak learner, such as a decision tree, to the negative gradient residuals.\n",
    "\n",
    "c. Multiply the predictions of the new model by a learning rate parameter and add them to the predictions of the previous models.\n",
    "\n",
    "Repeat step 2 until the desired number of models is reached.\n",
    "\n",
    "Make predictions by combining the predictions of all models in the ensemble.\n",
    "\n",
    "Calculate the loss function on the predictions and the true targets to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ed5aa-c2e5-4ac6-9e9e-632cbebaeab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
