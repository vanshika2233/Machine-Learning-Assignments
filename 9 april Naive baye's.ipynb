{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a1b60e-bb33-4b87-980f-0ffe4ada39de",
   "metadata": {},
   "source": [
    "Q1. What is Bayes' theorem?\n",
    "\n",
    "Bayes' theorem is a mathematical concept that describes the probability of an event occurring based on prior knowledge or information. It provides a way to update our beliefs about the probability of an event occurring as we gather new evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580a716-5043-49ce-92f4-d717444a6627",
   "metadata": {},
   "source": [
    "Q2. What is the formula for Bayes' theorem?\n",
    "\n",
    "The formula for Bayes' theorem is:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "where:\n",
    "\n",
    "P(A|B) is the probability of A given B\n",
    "\n",
    "P(B|A) is the probability of B given A\n",
    "\n",
    "P(A) is the prior probability of A\n",
    "\n",
    "P(B) is the prior probability of B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a119b-cbb6-4088-bdbf-e55e4d17a81f",
   "metadata": {},
   "source": [
    "Q3. How is Bayes' theorem used in practice?\n",
    "\n",
    "Bayes' theorem is used in a variety of fields, including statistics, machine learning, and artificial intelligence. It can be used to make predictions or classifications based on probabilities. For example, in spam filtering, Bayes' theorem can be used to calculate the probability that an email is spam based on the words and phrases it contains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9be796-e190-4924-8e82-9b72ccf85803",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "\n",
    "Bayes' theorem is related to conditional probability in that it describes the probability of an event A given event B. Conditional probability describes the probability of an event A given that another event B has occurred. Bayes' theorem is a way to update our beliefs about the probability of an event occurring based on new evidence or information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2117f51-7e02-4647-a2e1-f553aa865ecb",
   "metadata": {},
   "source": [
    "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "\n",
    "The choice of Naive Bayes classifier depends on the nature of the problem and the characteristics of the data. There are three types of Naive Bayes classifiers: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. Gaussian Naive Bayes is used for continuous data, Multinomial Naive Bayes is used for discrete data, and Bernoulli Naive Bayes is used for binary data. The choice of classifier depends on the type of data and the assumptions that can be made about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce120f3d-73ed-4e38-aa6c-6d490327ca44",
   "metadata": {},
   "source": [
    "Q6. Assignment: You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:\n",
    "\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "\n",
    "A 3 3 4 4 3 3 3\n",
    "\n",
    "B 2 2 1 2 2 2 3\n",
    "\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1bbb08-968d-4c3b-ae25-3d2443cdbd5d",
   "metadata": {},
   "source": [
    "The Naive Bayes assumption is that the features are conditionally independent given the class, so we can calculate the posterior probability for each class by multiplying the prior probability of the class by the product of the conditional probabilities of each feature given the class.\n",
    "\n",
    "The prior probability of each class is assumed to be equal, so P(A) = P(B) = 0.5.\n",
    "\n",
    "The conditional probability of each feature given the class can be estimated from the frequency table:\n",
    "\n",
    "P(X1=3|A) = 4/10 P(X2=4|A) = 3/10 P(X1=3|B) = 1/7 P(X2=4|B) = 1/7\n",
    "\n",
    "To calculate the posterior probability of class A given the observed features, we can use Bayes' rule:\n",
    "\n",
    "P(A|X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) / P(X1=3,X2=4)\n",
    "\n",
    "We can simplify this expression using the Naive Bayes assumption:\n",
    "\n",
    "P(X1=3,X2=4|A) = P(X1=3|A) * P(X2=4|A) = (4/10) * (3/10) = 12/100\n",
    "\n",
    "Similarly, we can calculate the posterior probability of class B:\n",
    "\n",
    "P(B|X1=3,X2=4) = P(X1=3,X2=4|B) * P(B) / P(X1=3,X2=4) P(X1=3,X2=4|B) = P(X1=3|B) * P(X2=4|B) = (1/7) * (1/7) = 1/49\n",
    "\n",
    "To calculate P(X1=3,X2=4), we can use the law of total probability:\n",
    "\n",
    "P(X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) + P(X1=3,X2=4|B) * P(B) = (12/100) * 0.5 + (1/49) * 0.5 = 0.0648\n",
    "\n",
    "Finally, we can calculate the normalized posterior probabilities:\n",
    "\n",
    "P(A|X1=3,X2=4) = (12/100) * 0.5 / 0.0648 = 0.465 P(B|X1=3,X2=4) = (1/49) * 0.5 / 0.0648 = 0.035\n",
    "\n",
    "Therefore, Naive Bayes would predict that the new instance belongs to class A, since it has a higher posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0ad7e-ba8e-4d04-8a23-6c3ba9a6e8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
