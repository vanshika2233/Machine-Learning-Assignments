{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4857bdfc-9e68-427d-9ba3-e011231e35d7",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "Lasso Regression is a linear regression technique that performs both regularization and feature selection. The name Lasso stands for \"Least Absolute Shrinkage and Selection Operator.\" It differs from other regression techniques in that it uses L1 regularization, which adds a penalty term to the cost function that constrains the sum of the absolute values of the coefficients. This penalty term helps to reduce the model complexity by shrinking the less important features' coefficients towards zero, effectively eliminating them from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc85e1-436a-4fe4-883e-a284842e9d0d",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "The main advantage of using Lasso Regression in feature selection is that it can handle a large number of features and automatically select the most important ones for the model. By reducing the number of features, Lasso Regression helps to prevent overfitting, improves the model's generalization performance, and reduces the computational resources required for training th"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc45c3a-7ac7-4b73-8255-59e4aafdccb8",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "The coefficients of a Lasso Regression model represent the weights assigned to each input feature. They indicate the direction and strength of the relationship between the feature and the target variable. The coefficients can be positive, negative, or zero, depending on the feature's importance for the model. A positive coefficient indicates a positive relationship, a negative coefficient indicates a negative relationship, and a zero coefficient indicates that the feature has been eliminated from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffa3b2f-8d86-41c1-bd74-5267908d22f8",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\n",
    "The tuning parameter in Lasso Regression is the regularization parameter, lambda. It controls the strength of the penalty term and determines the amount of shrinkage applied to the coefficients. A higher value of lambda results in stronger regularization, which leads to smaller coefficients and a simpler model. However, too much regularization can cause underfitting and reduce the model's predictive power. The optimal value of lambda can be determined using cross-validation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fead73-6e81-44a4-bca4-c33103df3e96",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "Lasso Regression can be used for non-linear regression problems by adding polynomial or interaction terms to the input features. However, the resulting model is still a linear combination of the input features, and the L1 regularization constraint is still applied to the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc06f2-ee58-49bf-a3b2-9e4e0a5ca26c",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "Ridge Regression and Lasso Regression both perform regularization, but they differ in the type of penalty term used. Ridge Regression uses L2 regularization, which adds a penalty term that constrains the sum of the squared values of the coefficients. This penalty term shrinks the coefficients towards zero but does not eliminate them entirely. In contrast, Lasso Regression uses L1 regularization, which adds a penalty term that constrains the sum of the absolute values of the coefficients. This penalty term can lead to some coefficients being exactly zero, effectively eliminating the corresponding features from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e58771-f286-4312-83ca-da0bce05ed76",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "Lasso Regression can handle multicollinearity in the input features by selecting only one of the highly correlated features and setting the coefficients of the others to zero. This effectively performs feature selection and removes the redundant features from the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0806576e-f4fe-41e6-bdb6-0f1de3a00bd3",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n",
    "The optimal value of the regularization parameter (lambda) in Lasso Regression can be chosen using cross-validation techniques. The dataset is divided into training and validation sets, and the model is trained on the training set with different values of lambda. The performance of the model is evaluated on the validation set, and the value of lambda that produces the best performance is selected as the optimal value. Alternatively, some automated methods, such as LassoCV, can be used to automatically select the best value of lambda based on the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f866dee-64c1-464a-9b94-a939d15d6c88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
